{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:\n",
    "Domain adaptation in the realm of supervised learning has been an interesting area in NLP. A paper by Hal from 2007  proposes a simple idea of augmenting the feature space of both the source as well as the target domain and use it as the input to a classifier which aims at learning a function to minimize the loss in the target domain.The idea was to take each feature in the source domain and make two version of it namely source-specific and generic.The same is done to the target domain with target-specific and generic.By augmenting the feature space, the algorithm learns to adapt on its own.This approach has been used well for tasks like parts of speech tagging, parsing. We would like to take this approach to sentiment analysis.\n",
    "\n",
    "    Mou et al have looked at the transferrability of neural networks in natural language processing in a paper in 2016. They looked at different tasks like sentence classification and also sentiment analysis. For sentiment analysis they looked at LSTM-RNN while CNNs were used for sentence classification.\n",
    "    There research focused on the following.\n",
    "RQ1: How transferable are neural networks between\n",
    "two tasks with similar or different semantics\n",
    "in NLP applications?\n",
    "RQ2: How transferable are different layers of NLP\n",
    "neural models?\n",
    "RQ3: How transferable are INIT and MULT, respectively?\n",
    "What is the effect of combining\n",
    "these two methods?\n",
    "The transferrability is dependent on how semantically the two tasks are.\n",
    "In particular, we learn that for sentiment analysis, tranferring and finetuning the embedding layer as well as the hidden layer helps in improving performance. The output layer is more specific to the datast and performs best when left initialized randomly. An interesting aspect of this paper is the question about when the parameters are ready to trasfer?The paper reported a sharp increase in accuracy from epochs 1-5 which plateaus later. We intend to leverage this in our work.\n",
    "\n",
    "Yet another interesting work which may lend itself to transfer learning in sentiment analysis has been that of deep contextualized word representation in a paper published this year by Clark el al. This paper explores word vectors each token is assigned a representation that is a function of the entire input sentence. The vectors are derived from a bidirectional LSTM that is trained with a coupled language model(LM) on a large text corpus. \n",
    "The word vectors are learned functions of the internal states of a deep bidirectional language model (biLM). These representations are called ELMo(Embeddings from a language model). This paper investigates the performance of ELMo on several tasks including sentiment analysis where it reports an improvement of 1% in accuracy over state of the art. Adding ELMo to a model increases the sample efficiencyconsiderably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size. Although, this paper does not allude to transfer learning, the efficiency gained here can be helpful when we want to minimize  training time  on source domain in order to be able to transfer sooner. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods: \n",
    "Can add the pseudo code from Hal paper if needed. https://stackoverflow.com/questions/34116254/cannot-understand-a-short-simple-algorithm-in-perl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
